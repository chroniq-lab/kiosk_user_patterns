{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ae2536",
   "metadata": {},
   "source": [
    "## 1. Extract Monthly Patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da3ef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from config import PATH_KIOSK_USER_PATTERNS_FOLDER, PATH_KIOSK_USER_PATTERNS_REPO, PATH_SSD_ADVAN_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b94ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "execfile(Path(PATH_KIOSK_USER_PATTERNS_REPO, \"functions/safe_parse_json.py\"))\n",
    "\n",
    "cols = [\n",
    "    \"PLACEKEY\", \"LOCATION_NAME\", \"NAICS_CODE\", \"LATITUDE\", \"LONGITUDE\",\n",
    "    \"STREET_ADDRESS\", \"CITY\", \"REGION\", \"DATE_RANGE_START\", \"DATE_RANGE_END\",\n",
    "    \"RAW_VISITOR_COUNTS\", \"RAW_VISIT_COUNTS\", \"VISITOR_HOME_CBGS\"\n",
    "]\n",
    "monthly_patterns_files = list((PATH_SSD_ADVAN_FOLDER / \"Monthly Patterns\" / \"Foot Traffic\").rglob('**/*.gz'))\n",
    "len(monthly_patterns_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f2ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract year from file path using regex (assuming year is a 4-digit number in the path)\n",
    "monthly_patterns_files_df = pd.DataFrame({\n",
    "    'file_path': monthly_patterns_files,\n",
    "    'FILE_NAME': [f.name for f in monthly_patterns_files],\n",
    "    'YEAR': [int(re.search(r'(\\d{4})', str(f)).group(1)) if re.search(r'(\\d{4})', str(f)) else None for f in monthly_patterns_files]\n",
    "})\n",
    "\n",
    "pd.options.display.max_colwidth = None  # Show full file paths in the DataFrame\n",
    "monthly_patterns_files_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc1225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_base = PATH_KIOSK_USER_PATTERNS_FOLDER / \"working/processed/kupdat03_advan research monthly patterns\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684461dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds.dataset(output_base, format=\"parquet\", partitioning=[\"YEAR\", \"MONTH\"])\n",
    "\n",
    "# Check if the dataset exists and get unique file names\n",
    "try:\n",
    "    # Use PyArrow to read the metadata\n",
    "    table = dataset.to_table(columns=[\"FILE_NAME\",\"YEAR\"])\n",
    "    # Convert to pandas to get unique values\n",
    "    unique_files = table.to_pandas().drop_duplicates().reset_index(drop=True)\n",
    "    # Extract the year as integer from the \"YEAR\" column (e.g., \"YEAR=2023\" -> 2023)\n",
    "    unique_files['YEAR'] = unique_files['YEAR'].str.extract(r'(\\d{4})').astype(int)\n",
    "    print(f\"Found {len(unique_files['FILE_NAME'])} unique files in the dataset\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing the dataset: {e}\")\n",
    "    # Check if the directory exists\n",
    "    if not output_base.exists():\n",
    "        print(f\"Directory does not exist: {output_base}\")\n",
    "\n",
    "\n",
    "unique_files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f2c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = monthly_patterns_files_df.merge(\n",
    "        unique_files, \n",
    "        left_on=['FILE_NAME', 'YEAR'], \n",
    "        right_on=['FILE_NAME', 'YEAR'], \n",
    "        how='left',\n",
    "        indicator=True\n",
    "    )\n",
    "    \n",
    "    # Keep only rows that don't have a match\n",
    "unprocessed = merged[merged['_merge'] == 'left_only']\n",
    "\n",
    "unprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976dcaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed.iloc[0]['file_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79fdf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_list = []\n",
    "\n",
    "import json\n",
    "\n",
    "for f in unprocessed['file_path']:\n",
    "    # Read the gzip-compressed CSV file\n",
    "    print(f\"Processing file: {f}\")\n",
    "    \n",
    "    try:\n",
    "        df_table = pd.read_csv(f, \n",
    "                              compression='gzip', \n",
    "                              usecols=cols, \n",
    "                              engine='python',\n",
    "                              on_bad_lines='skip',\n",
    "                              \n",
    "                              encoding= 'latin1')\n",
    "\n",
    "        # Filter rows where LOCATION_NAME contains 'walmart'\n",
    "        walmart_mask = df_table['LOCATION_NAME'].str.contains('walmart', case=False, na=False)\n",
    "        df_walmart = (df_table[walmart_mask]\n",
    "                      .assign(\n",
    "                          DATE_RANGE_START=lambda x: pd.to_datetime(x['DATE_RANGE_START']))\n",
    "                      .assign(YEAR =lambda x: x['DATE_RANGE_START'].dt.year,\n",
    "                              MONTH=lambda x: x['DATE_RANGE_START'].dt.month,\n",
    "                              FILE_NAME=f.name))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # Expand VISITOR_HOME_CBGS into long format\n",
    "        df_cbgs = (df_walmart.dropna(subset=['VISITOR_HOME_CBGS']).copy())\n",
    "        \n",
    "        df_cbgs = (df_cbgs\n",
    "                   .assign(VISITOR_HOME_CBGS=df_cbgs['VISITOR_HOME_CBGS'].apply(lambda x: json.loads(x) if isinstance(x, str) else {})\n",
    "        ).assign(VISITOR_HOME_CBGS=df_cbgs['VISITOR_HOME_CBGS'].apply(safe_parse_json)\n",
    "        ).assign(VISITOR_HOME_CBGS=df_cbgs['VISITOR_HOME_CBGS'].apply(lambda x: list(x.items()) if isinstance(x, dict) else None))\n",
    "        )\n",
    "\n",
    "        # Then explode\n",
    "        df_long = df_cbgs.explode('VISITOR_HOME_CBGS')\n",
    "        # df_long = df_long.reset_index(drop=True)\n",
    "\n",
    "        df_long = (df_long\n",
    "            .reset_index(drop=True)\n",
    "            .assign(\n",
    "                HOME_CBG=lambda x: x['VISITOR_HOME_CBGS'].apply(lambda y: y[0] if isinstance(y, tuple) else None),\n",
    "                VISITOR_COUNT=lambda x: x['VISITOR_HOME_CBGS'].apply(lambda y: y[1] if isinstance(y, tuple) else None)\n",
    "            ).drop(columns=['VISITOR_HOME_CBGS'])\n",
    "        )\n",
    "        # Save to Parquet with partitioning by YEAR and MONTH\n",
    "        df_long.to_parquet(\n",
    "            output_base,\n",
    "            index=False,\n",
    "            partition_cols=[\"YEAR\", \"MONTH\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {f}: {e}\")\n",
    "        error_list.append(f)\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98286004",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(error_list).to_csv(PATH_KIOSK_USER_PATTERNS_REPO / \"data/kupdat03_advan research monthly patterns errors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf40e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_walmart.iloc[0]['FILE_NAME']  # Display the file name of the first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb89c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7018d140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionaries to lists of key-value pairs\n",
    "df_cbgs['VISITOR_HOME_CBGS'] = df_cbgs['VISITOR_HOME_CBGS'].apply(\n",
    "    lambda x: list(x.items()) if isinstance(x, dict) else None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25d4fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds.dataset(output_base, format=\"parquet\")\n",
    "\n",
    "df = next(dataset.to_batches(batch_size=10)).to_pandas()\n",
    "df.head(n = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4df9cc",
   "metadata": {},
   "source": [
    "## 2. Extract Home Panel Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fde541",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_panel_files = list((PATH_SSD_ADVAN_FOLDER / \"Monthly Patterns\" / \"Home Panel Summary\").rglob('**/*.gz'))\n",
    "len(home_panel_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d207cc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year from file path using regex (assuming year is a 4-digit number in the path)\n",
    "home_panel_files_df = pd.DataFrame({\n",
    "    'file_path': home_panel_files,\n",
    "    'FILE_NAME': [f.name for f in home_panel_files]\n",
    "})\n",
    "\n",
    "pd.options.display.max_colwidth = None  # Show full file paths in the DataFrame\n",
    "home_panel_files_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1879dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_base_home_panel = PATH_KIOSK_USER_PATTERNS_FOLDER / \"working/processed/kupdat03_advan research home panel summary\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d65c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_list = []\n",
    "\n",
    "\n",
    "for f in home_panel_files_df['file_path']:\n",
    "    # Read the gzip-compressed CSV file\n",
    "    print(f\"Processing file: {f}\")\n",
    "    \n",
    "    try:\n",
    "        df_table = pd.read_csv(f, \n",
    "                              compression='gzip', \n",
    "                              engine='python',\n",
    "                              on_bad_lines='skip')\n",
    "\n",
    "        # Create a dictionary mapping state abbreviations to full names\n",
    "        state_abbrev_to_name = {\n",
    "            'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n",
    "            'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n",
    "            'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa',\n",
    "            'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n",
    "            'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri',\n",
    "            'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey',\n",
    "            'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio',\n",
    "            'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',\n",
    "            'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont',\n",
    "            'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming',\n",
    "            'DC': 'District of Columbia', 'PR': 'Puerto Rico', 'VI': 'Virgin Islands', 'GU': 'Guam',\n",
    "            'AS': 'American Samoa', 'MP': 'Northern Mariana Islands'\n",
    "        }\n",
    "        \n",
    "        df_usa = (df_table[df_table['ISO_COUNTRY_CODE'] == 'US']\n",
    "              .rename(columns={'MON': 'MONTH'})\n",
    "              .assign(FILE_NAME = lambda x: f.name)\n",
    "              .assign(state_name = lambda x: x['REGION'].map(state_abbrev_to_name)))\n",
    "        \n",
    "\n",
    "        \n",
    "        # Print data summary\n",
    "        print(f\"Found {len(df_usa)} USA records\")\n",
    "        # Save to Parquet with partitioning by YEAR and MONTH\n",
    "        df_usa.to_parquet(\n",
    "            output_base_home_panel,\n",
    "            index=False,\n",
    "            partition_cols=[\"YEAR\", \"MONTH\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {f}: {e}\")\n",
    "        error_list.append(f)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5177d644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
